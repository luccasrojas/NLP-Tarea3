{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multy-Domain Sentiment Dataset: Sentiment Analisys per domain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Preprocesamiento\n",
    "Para el preprocesamiento, se creó la clase \"Corpus\" que recibe una serie de documentos y obtiene las etiquetas de cada uno, el vocabulario, y las representaciones tf y tfidf."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "from scipy.sparse import coo_array\n",
    "\n",
    "POSITIVE_LABEL = 1\n",
    "NEGATIVE_LABEL = 0\n",
    "\n",
    "CATEGORY_PATHS = {\n",
    "    \"books\": \"../data/books/\",\n",
    "    \"dvd\": \"../data/dvd/\",\n",
    "    \"electronics\": \"../data/electronics/\",\n",
    "    \"kitchen\": \"../data/kitchen/\",\n",
    "}\n",
    "\n",
    "\n",
    "class Corpus:\n",
    "    # Atributes\n",
    "    __vocabulary = {}\n",
    "    __tf = None\n",
    "    __tfidf = None\n",
    "    __documents = None\n",
    "    __doc_frequencies = None\n",
    "\n",
    "\n",
    "    def __init__(self, docs: list, vocabulary: dict = None) -> None:\n",
    "        \"\"\"\n",
    "        Creates the vocabulary, the TF and the TFIDF matricees from docs.\n",
    "        \n",
    "            Params\n",
    "            ------\n",
    "                docs: pd.DataFrame\n",
    "                    List of documents.\n",
    "                \n",
    "                vocabulary: dict | None (default: None)\n",
    "                    A dictionary where the keys are the terms in the vocabulary\n",
    "                    and the values are each term's unique id. If set to None,\n",
    "                    this class will create the vocabulary based on docs.\n",
    "                    WARNING: if specified, vocabulary must contain a \"#UNK#\" key\n",
    "                    for unknown terms.\n",
    "        \"\"\"        \n",
    "        self.__documents = self.__get_docs_df(docs)\n",
    "        if vocabulary is None:\n",
    "            self.__vocabulary = self.__load_vocabulary()\n",
    "        else:\n",
    "            self.__vocabulary = vocabulary\n",
    "            \n",
    "        self.__tf = self.__load_tf()\n",
    "        self.__tfidf = self.__load_tfidf()\n",
    "\n",
    "    def __get_docs_df(self, docs: list):\n",
    "        documents = []\n",
    "        for doc in docs:\n",
    "            terms = self.__get_term_counts(doc)\n",
    "            label = self.__get_label(doc)\n",
    "            documents.append([terms, label])\n",
    "        \n",
    "        return pd.DataFrame(documents, columns=[\"terms\", \"label\"])\n",
    "\n",
    "    def __get_term_counts(self, line: str) -> list:\n",
    "        line_arr = line.split()\n",
    "        line_arr = line_arr[:-1] # Remove #label#: from the end of array\n",
    "        terms = {}\n",
    "        for term in line_arr:\n",
    "            term_arr = term.split(\":\")\n",
    "            terms[term_arr[0]] = int(term_arr[1])\n",
    "        \n",
    "        return terms\n",
    "    \n",
    "    def __get_label(self, doc: str) -> int:\n",
    "        doc_arr = doc.split()\n",
    "        label_str = doc_arr[-1]\n",
    "        label = label_str.split(\":\")[-1]\n",
    "        if label.lower() == \"negative\":\n",
    "            return NEGATIVE_LABEL\n",
    "        elif label.lower() == \"positive\":\n",
    "            return POSITIVE_LABEL\n",
    "    \n",
    "    def __load_vocabulary(self):\n",
    "        # Term counts in the whole corpus\n",
    "        voc = {}\n",
    "        docs = self.__documents\n",
    "        for i in range(len(docs)):\n",
    "            terms_dict = docs.loc[i, \"terms\"]\n",
    "            for term in terms_dict:\n",
    "                if term in voc:\n",
    "                    voc[term] += terms_dict[term]\n",
    "                else:\n",
    "                    voc[term] = terms_dict[term]\n",
    "        \n",
    "        # Replace terms with one appearance with UNK\n",
    "        terms_to_del = []\n",
    "        for term in voc:\n",
    "            if voc[term] == 1:\n",
    "                terms_to_del.append(term)\n",
    "\n",
    "        # Remove terms that appeare only once\n",
    "        for term in terms_to_del:\n",
    "            voc.pop(term)\n",
    "        \n",
    "        # Assign unique ids to terms in vocabulary\n",
    "        bow = {\"#UNK#\": 0}\n",
    "        i = 1\n",
    "        for term in voc:\n",
    "            bow[term] = i\n",
    "            i += 1\n",
    "        \n",
    "        return bow\n",
    "\n",
    "    def __load_tf(self) -> np.ndarray:\n",
    "        voc = self.__vocabulary\n",
    "        docs = self.__documents\n",
    "        data = []\n",
    "        x = []\n",
    "        y = []\n",
    "        for i in range(len(docs)):\n",
    "            doc = docs.iloc[i]\n",
    "            for term in doc[\"terms\"]:\n",
    "                if term in voc:\n",
    "                    data.append(doc[\"terms\"][term])\n",
    "                    x.append(i)\n",
    "                    y.append(voc[term])\n",
    "                else:\n",
    "                    data.append(doc[\"terms\"][term])\n",
    "                    x.append(i)\n",
    "                    y.append(voc[\"#UNK#\"])\n",
    "        \n",
    "        sparce_matrix = coo_array((data, (x,y)), shape=(len(docs), len(voc)), dtype=np.uint64).tocsr()\n",
    "        return sparce_matrix\n",
    "\n",
    "    def __load_tfidf(self) -> np.ndarray:\n",
    "        voc = self.__vocabulary\n",
    "        docs = self.__documents\n",
    "        tfs = self.__tf\n",
    "        doc_frec = np.zeros(len(voc))\n",
    "        # Calculate document frequencies\n",
    "        for i in range(len(docs)):\n",
    "            doc = docs.loc[i, \"terms\"]\n",
    "            added_unk = False\n",
    "            for term in doc:\n",
    "                if term in voc:\n",
    "                    term_id = voc[term]\n",
    "                    doc_frec[term_id] += 1\n",
    "                elif not added_unk:\n",
    "                    term_id = voc[\"#UNK#\"]\n",
    "                    doc_frec[term_id] += 1\n",
    "                    added_unk = True\n",
    "\n",
    "        total_docs = len(docs)\n",
    "        data = []\n",
    "        x = []\n",
    "        y = []\n",
    "        for i in range(len(docs)):\n",
    "            doc = docs.iloc[i]\n",
    "            unk_tf = 0\n",
    "            for term in doc[\"terms\"]:\n",
    "                if term in voc:\n",
    "                    term_id = voc[term]\n",
    "                    tf = doc[\"terms\"][term]\n",
    "                    df = int(doc_frec[term_id])\n",
    "                    data.append(math.log10(1+tf) * math.log10(total_docs / df))\n",
    "                    x.append(i)\n",
    "                    y.append(term_id)\n",
    "                else:\n",
    "                    unk_tf += doc[\"terms\"][term]\n",
    "            \n",
    "            # Calculate UNK tfidf\n",
    "            term_id = voc[\"#UNK#\"]\n",
    "            tf = int(tfs[i, term_id])\n",
    "            df = int(doc_frec[term_id])\n",
    "            temp = math.log10(1+tf) * math.log10(total_docs / df)\n",
    "            if temp < 0:\n",
    "                raise Exception(\"HP\")\n",
    "            data.append(math.log10(1+tf) * math.log10(total_docs / df))\n",
    "            x.append(i)\n",
    "            y.append(term_id)\n",
    "\n",
    "        sparce_matrix = coo_array((data, (x, y)), shape=(len(docs), len(voc)), dtype=np.float64).tocsr()        \n",
    "        return sparce_matrix\n",
    "\n",
    "    def getVocabulary(self) -> dict:\n",
    "        return self.__vocabulary\n",
    "    \n",
    "    def getDocuments(self) -> pd.DataFrame:\n",
    "        return self.__documents\n",
    "\n",
    "    def getTfs(self):\n",
    "        return self.__tf\n",
    "    \n",
    "    def getTfidfs(self):\n",
    "        return self.__tfidf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Construcción del modelo y métricas por categoría"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_metrics(labels: list, predicted_labels: list) -> dict:\n",
    "    tp = 0\n",
    "    fp = 0\n",
    "    fn = 0\n",
    "    tn = 0\n",
    "\n",
    "    for i in range(len(labels)):\n",
    "        act     = labels[i]\n",
    "        pred    = predicted_labels[i]\n",
    "        if(act == 1 and pred == 1):\n",
    "            tp += 1\n",
    "        elif pred == 1 and act == 0:\n",
    "            fp += 1\n",
    "        elif pred == 0 and act == 1:\n",
    "            fn += 1\n",
    "        elif pred == 0 and act == 0:\n",
    "            tn += 1\n",
    "    \n",
    "    presition = tp / (tp + fp)\n",
    "    recall = tp / (tp + fn)\n",
    "    f1 =  2 * presition * recall / (presition + recall)\n",
    "\n",
    "    return {\"presition\": presition, \"recall\": recall, \"f1\": f1}\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "def test_nv_classifier(X_train: np.ndarray, y_train, X_test: np.ndarray, y_test) -> dict:\n",
    "    model = MultinomialNB()\n",
    "    model.fit(X_train, y_train)\n",
    "    y_predicted = model.predict(X_test)\n",
    "    return calculate_metrics(y_test, y_predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def get_docs_from_folder_paths(folder_paths: list):\n",
    "    train_docs = []\n",
    "    test_docs = []\n",
    "    for path in folder_paths:\n",
    "        positive_path = os.path.join(path, \"positive.review\")\n",
    "        negative_path = os.path.join(path, \"negative.review\")\n",
    "        test_path = os.path.join(path, \"unlabeled.review\")\n",
    "\n",
    "        f = open(positive_path)\n",
    "        line = f.readline()\n",
    "        while line != \"\":\n",
    "            train_docs.append(line)\n",
    "            line = f.readline()\n",
    "        f.close()\n",
    "\n",
    "        f = open(negative_path)\n",
    "        line = f.readline()\n",
    "        while line != \"\":\n",
    "            train_docs.append(line)\n",
    "            line = f.readline()\n",
    "        f.close()\n",
    "\n",
    "        f = open(test_path)\n",
    "        line = f.readline()\n",
    "        while line != \"\":\n",
    "            test_docs.append(line)\n",
    "            line = f.readline()\n",
    "        f.close()\n",
    "\n",
    "    return train_docs, test_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_multinomial_nv_by_cat():\n",
    "    for cat in CATEGORY_PATHS:\n",
    "        print(\"=== Multinomial NV for:\", cat, \"===\")\n",
    "        folder_path = CATEGORY_PATHS[cat]\n",
    "        train_docs, test_docs = get_docs_from_folder_paths([folder_path])\n",
    "        train_corpus = Corpus(train_docs)\n",
    "        test_corpus = Corpus(test_docs, train_corpus.getVocabulary())\n",
    "        y_train = train_corpus.getDocuments()[\"label\"]\n",
    "        y_test = test_corpus.getDocuments()[\"label\"]\n",
    "\n",
    "        # Test with TFs\n",
    "        print(\"TFs: \", end=\"\")\n",
    "        X_train = train_corpus.getTfs()\n",
    "        X_test = test_corpus.getTfs()\n",
    "        print(test_nv_classifier(X_train, y_train, X_test, y_test))\n",
    "\n",
    "        # Test with TFIDFs\n",
    "        print(\"TFIDFs: \", end=\"\")\n",
    "        X_train = train_corpus.getTfidfs()\n",
    "        X_test = test_corpus.getTfidfs()\n",
    "        print(test_nv_classifier(X_train, y_train, X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Multinomial NV for: books ===\n",
      "TFs: {'presition': 0.8158333333333333, 'recall': 0.8648409893992933, 'f1': 0.8396226415094339}\n",
      "TFIDFs: {'presition': 0.827893175074184, 'recall': 0.8626325088339223, 'f1': 0.8449059052563271}\n",
      "=== Multinomial NV for: dvd ===\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\juanc\\Documents\\Repos\\Uniandes\\202320\\NLP\\NLP-Tarea3\\Punto2\\src\\MDSD-SentimentAnalisys.ipynb Cell 9\u001b[0m line \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/juanc/Documents/Repos/Uniandes/202320/NLP/NLP-Tarea3/Punto2/src/MDSD-SentimentAnalisys.ipynb#X11sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m test_multinomial_nv_by_cat()\n",
      "\u001b[1;32mc:\\Users\\juanc\\Documents\\Repos\\Uniandes\\202320\\NLP\\NLP-Tarea3\\Punto2\\src\\MDSD-SentimentAnalisys.ipynb Cell 9\u001b[0m line \u001b[0;36m7\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/juanc/Documents/Repos/Uniandes/202320/NLP/NLP-Tarea3/Punto2/src/MDSD-SentimentAnalisys.ipynb#X11sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m train_docs, test_docs \u001b[39m=\u001b[39m get_docs_from_folder_paths([folder_path])\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/juanc/Documents/Repos/Uniandes/202320/NLP/NLP-Tarea3/Punto2/src/MDSD-SentimentAnalisys.ipynb#X11sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m train_corpus \u001b[39m=\u001b[39m Corpus(train_docs)\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/juanc/Documents/Repos/Uniandes/202320/NLP/NLP-Tarea3/Punto2/src/MDSD-SentimentAnalisys.ipynb#X11sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m test_corpus \u001b[39m=\u001b[39m Corpus(test_docs, train_corpus\u001b[39m.\u001b[39mgetVocabulary())\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/juanc/Documents/Repos/Uniandes/202320/NLP/NLP-Tarea3/Punto2/src/MDSD-SentimentAnalisys.ipynb#X11sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m y_train \u001b[39m=\u001b[39m train_corpus\u001b[39m.\u001b[39mgetDocuments()[\u001b[39m\"\u001b[39m\u001b[39mlabel\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/juanc/Documents/Repos/Uniandes/202320/NLP/NLP-Tarea3/Punto2/src/MDSD-SentimentAnalisys.ipynb#X11sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m y_test \u001b[39m=\u001b[39m test_corpus\u001b[39m.\u001b[39mgetDocuments()[\u001b[39m\"\u001b[39m\u001b[39mlabel\u001b[39m\u001b[39m\"\u001b[39m]\n",
      "\u001b[1;32mc:\\Users\\juanc\\Documents\\Repos\\Uniandes\\202320\\NLP\\NLP-Tarea3\\Punto2\\src\\MDSD-SentimentAnalisys.ipynb Cell 9\u001b[0m line \u001b[0;36m5\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/juanc/Documents/Repos/Uniandes/202320/NLP/NLP-Tarea3/Punto2/src/MDSD-SentimentAnalisys.ipynb#X11sZmlsZQ%3D%3D?line=46'>47</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m__vocabulary \u001b[39m=\u001b[39m vocabulary\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/juanc/Documents/Repos/Uniandes/202320/NLP/NLP-Tarea3/Punto2/src/MDSD-SentimentAnalisys.ipynb#X11sZmlsZQ%3D%3D?line=48'>49</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m__tf \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m__load_tf()\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/juanc/Documents/Repos/Uniandes/202320/NLP/NLP-Tarea3/Punto2/src/MDSD-SentimentAnalisys.ipynb#X11sZmlsZQ%3D%3D?line=49'>50</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m__tfidf \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m__load_tfidf()\n",
      "\u001b[1;32mc:\\Users\\juanc\\Documents\\Repos\\Uniandes\\202320\\NLP\\NLP-Tarea3\\Punto2\\src\\MDSD-SentimentAnalisys.ipynb Cell 9\u001b[0m line \u001b[0;36m1\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/juanc/Documents/Repos/Uniandes/202320/NLP/NLP-Tarea3/Punto2/src/MDSD-SentimentAnalisys.ipynb#X11sZmlsZQ%3D%3D?line=164'>165</a>\u001b[0m         y\u001b[39m.\u001b[39mappend(term_id)\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/juanc/Documents/Repos/Uniandes/202320/NLP/NLP-Tarea3/Punto2/src/MDSD-SentimentAnalisys.ipynb#X11sZmlsZQ%3D%3D?line=165'>166</a>\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> <a href='vscode-notebook-cell:/c%3A/Users/juanc/Documents/Repos/Uniandes/202320/NLP/NLP-Tarea3/Punto2/src/MDSD-SentimentAnalisys.ipynb#X11sZmlsZQ%3D%3D?line=166'>167</a>\u001b[0m         unk_tf \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m doc[\u001b[39m\"\u001b[39m\u001b[39mterms\u001b[39m\u001b[39m\"\u001b[39m][term]\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/juanc/Documents/Repos/Uniandes/202320/NLP/NLP-Tarea3/Punto2/src/MDSD-SentimentAnalisys.ipynb#X11sZmlsZQ%3D%3D?line=168'>169</a>\u001b[0m \u001b[39m# Calculate UNK tfidf\u001b[39;00m\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/juanc/Documents/Repos/Uniandes/202320/NLP/NLP-Tarea3/Punto2/src/MDSD-SentimentAnalisys.ipynb#X11sZmlsZQ%3D%3D?line=169'>170</a>\u001b[0m term_id \u001b[39m=\u001b[39m voc[\u001b[39m\"\u001b[39m\u001b[39m#UNK#\u001b[39m\u001b[39m\"\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\juanc\\anaconda3\\envs\\nlpEnv\\Lib\\site-packages\\pandas\\core\\series.py:1007\u001b[0m, in \u001b[0;36mSeries.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   1004\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_values[key]\n\u001b[0;32m   1006\u001b[0m \u001b[39melif\u001b[39;00m key_is_scalar:\n\u001b[1;32m-> 1007\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_value(key)\n\u001b[0;32m   1009\u001b[0m \u001b[39mif\u001b[39;00m is_hashable(key):\n\u001b[0;32m   1010\u001b[0m     \u001b[39m# Otherwise index.get_value will raise InvalidIndexError\u001b[39;00m\n\u001b[0;32m   1011\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m   1012\u001b[0m         \u001b[39m# For labels that don't resolve as scalars like tuples and frozensets\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\juanc\\anaconda3\\envs\\nlpEnv\\Lib\\site-packages\\pandas\\core\\series.py:1119\u001b[0m, in \u001b[0;36mSeries._get_value\u001b[1;34m(self, label, takeable)\u001b[0m\n\u001b[0;32m   1116\u001b[0m loc \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mindex\u001b[39m.\u001b[39mget_loc(label)\n\u001b[0;32m   1118\u001b[0m \u001b[39mif\u001b[39;00m is_integer(loc):\n\u001b[1;32m-> 1119\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_values[loc]\n\u001b[0;32m   1121\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mindex, MultiIndex):\n\u001b[0;32m   1122\u001b[0m     mi \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mindex\n",
      "File \u001b[1;32mc:\\Users\\juanc\\anaconda3\\envs\\nlpEnv\\Lib\\site-packages\\pandas\\core\\series.py:750\u001b[0m, in \u001b[0;36mSeries._values\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    718\u001b[0m \u001b[39m@property\u001b[39m\n\u001b[0;32m    719\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_values\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m    720\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    721\u001b[0m \u001b[39m    Return the internal repr of this data (defined by Block.interval_values).\u001b[39;00m\n\u001b[0;32m    722\u001b[0m \u001b[39m    This are the values as stored in the Block (ndarray or ExtensionArray\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    748\u001b[0m \n\u001b[0;32m    749\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 750\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_mgr\u001b[39m.\u001b[39minternal_values()\n",
      "File \u001b[1;32mc:\\Users\\juanc\\anaconda3\\envs\\nlpEnv\\Lib\\site-packages\\pandas\\core\\internals\\managers.py:2011\u001b[0m, in \u001b[0;36mSingleBlockManager.internal_values\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   2008\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"The array that Series.values returns\"\"\"\u001b[39;00m\n\u001b[0;32m   2009\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_block\u001b[39m.\u001b[39mexternal_values()\n\u001b[1;32m-> 2011\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39minternal_values\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m   2012\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"The array that Series._values returns\"\"\"\u001b[39;00m\n\u001b[0;32m   2013\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_block\u001b[39m.\u001b[39mvalues\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "test_multinomial_nv_by_cat()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "def test_lr_classifier(X_train: np.ndarray, y_train, X_test: np.ndarray, y_test) -> dict:\n",
    "    model = LogisticRegression()\n",
    "    model.fit(X_train, y_train)\n",
    "    y_predicted = model.predict(X_test)\n",
    "    return calculate_metrics(y_test, y_predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_lr_classifier_by_cat():\n",
    "    for cat in CATEGORY_PATHS:\n",
    "        print(\"=== LR for:\", cat, \"===\")\n",
    "        folder_path = CATEGORY_PATHS[cat]\n",
    "        train_docs, test_docs = get_docs_from_folder_paths([folder_path])\n",
    "        train_corpus = Corpus(train_docs)\n",
    "        test_corpus = Corpus(test_docs, train_corpus.getVocabulary())\n",
    "        y_train = train_corpus.getDocuments()[\"label\"]\n",
    "        y_test = test_corpus.getDocuments()[\"label\"]\n",
    "\n",
    "        # Test with TFs\n",
    "        print(\"TFs: \", end=\"\")\n",
    "        X_train = train_corpus.getTfs().toarray()\n",
    "        X_test = test_corpus.getTfs()\n",
    "        print(test_lr_classifier(X_train, y_train, X_test, y_test))\n",
    "\n",
    "        # Test with TFIDFs\n",
    "        print(\"TFIDFs: \", end=\"\")\n",
    "        X_train = train_corpus.getTfidfs().toarray()\n",
    "        X_test = test_corpus.getTfidfs()\n",
    "        print(test_lr_classifier(X_train, y_train, X_test, y_test))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== LR for: books ===\n",
      "TFs: "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\juanc\\anaconda3\\envs\\nlpEnv\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'presition': 0.8192617734408146, 'recall': 0.8529151943462897, 'f1': 0.835749837697468}\n",
      "TFIDFs: {'presition': 0.8560965101249461, 'recall': 0.8776501766784452, 'f1': 0.8667393675027263}\n",
      "=== LR for: dvd ===\n"
     ]
    }
   ],
   "source": [
    "test_lr_classifier_by_cat()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Construcción del modelo y métricas con un compilado de todas las categorías"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_nv_classifier_complete():\n",
    "    print(\"=== TEST NV FOR FULL CORPUS ===\")\n",
    "    folder_paths = []\n",
    "    for cat in CATEGORY_PATHS:\n",
    "        folder_paths.append(CATEGORY_PATHS[cat])\n",
    "\n",
    "    train_docs, test_docs = get_docs_from_folder_paths(folder_paths)\n",
    "    train_corpus = Corpus(train_docs)\n",
    "    test_corpus = Corpus(test_docs, train_corpus.getVocabulary())\n",
    "    y_train = train_corpus.getDocuments()[\"label\"]\n",
    "    y_test = test_corpus.getDocuments()[\"label\"]\n",
    "\n",
    "    # Test with TFs\n",
    "    print(\"TFs: \", end=\"\")\n",
    "    X_train = train_corpus.getTfs()\n",
    "    X_test = test_corpus.getTfs()\n",
    "    print(test_lr_classifier(X_train, y_train, X_test, y_test))\n",
    "\n",
    "    # Test with TFIDFs\n",
    "    print(\"TFIDFs: \", end=\"\")\n",
    "    X_train = train_corpus.getTfidfs()\n",
    "    X_test = test_corpus.getTfidfs()\n",
    "    print(test_lr_classifier(X_train, y_train, X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== TEST NV FOR FULL CORPUS ===\n"
     ]
    },
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 7.39 GiB for an array with shape (8000, 123996) and data type float64",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\juanc\\Documents\\Repos\\Uniandes\\202320\\NLP\\NLP-Tarea3\\Punto2\\src\\MDSD-SentimentAnalisys.ipynb Cell 15\u001b[0m line \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/juanc/Documents/Repos/Uniandes/202320/NLP/NLP-Tarea3/Punto2/src/MDSD-SentimentAnalisys.ipynb#X24sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m test_nv_classifier_complete()\n",
      "\u001b[1;32mc:\\Users\\juanc\\Documents\\Repos\\Uniandes\\202320\\NLP\\NLP-Tarea3\\Punto2\\src\\MDSD-SentimentAnalisys.ipynb Cell 15\u001b[0m line \u001b[0;36m8\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/juanc/Documents/Repos/Uniandes/202320/NLP/NLP-Tarea3/Punto2/src/MDSD-SentimentAnalisys.ipynb#X24sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m     folder_paths\u001b[39m.\u001b[39mappend(CATEGORY_PATHS[cat])\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/juanc/Documents/Repos/Uniandes/202320/NLP/NLP-Tarea3/Punto2/src/MDSD-SentimentAnalisys.ipynb#X24sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m train_docs, test_docs \u001b[39m=\u001b[39m get_docs_from_folder_paths(folder_paths)\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/juanc/Documents/Repos/Uniandes/202320/NLP/NLP-Tarea3/Punto2/src/MDSD-SentimentAnalisys.ipynb#X24sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m train_corpus \u001b[39m=\u001b[39m Corpus(train_docs)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/juanc/Documents/Repos/Uniandes/202320/NLP/NLP-Tarea3/Punto2/src/MDSD-SentimentAnalisys.ipynb#X24sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m test_corpus \u001b[39m=\u001b[39m Corpus(test_docs, train_corpus\u001b[39m.\u001b[39mgetVocabulary())\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/juanc/Documents/Repos/Uniandes/202320/NLP/NLP-Tarea3/Punto2/src/MDSD-SentimentAnalisys.ipynb#X24sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m y_train \u001b[39m=\u001b[39m train_corpus\u001b[39m.\u001b[39mgetDocuments()[\u001b[39m\"\u001b[39m\u001b[39mlabel\u001b[39m\u001b[39m\"\u001b[39m]\n",
      "\u001b[1;32mc:\\Users\\juanc\\Documents\\Repos\\Uniandes\\202320\\NLP\\NLP-Tarea3\\Punto2\\src\\MDSD-SentimentAnalisys.ipynb Cell 15\u001b[0m line \u001b[0;36m4\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/juanc/Documents/Repos/Uniandes/202320/NLP/NLP-Tarea3/Punto2/src/MDSD-SentimentAnalisys.ipynb#X24sZmlsZQ%3D%3D?line=44'>45</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m__vocabulary \u001b[39m=\u001b[39m vocabulary\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/juanc/Documents/Repos/Uniandes/202320/NLP/NLP-Tarea3/Punto2/src/MDSD-SentimentAnalisys.ipynb#X24sZmlsZQ%3D%3D?line=46'>47</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m__tf \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m__load_tf()\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/juanc/Documents/Repos/Uniandes/202320/NLP/NLP-Tarea3/Punto2/src/MDSD-SentimentAnalisys.ipynb#X24sZmlsZQ%3D%3D?line=47'>48</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m__tfidf \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m__load_tfidf()\n",
      "\u001b[1;32mc:\\Users\\juanc\\Documents\\Repos\\Uniandes\\202320\\NLP\\NLP-Tarea3\\Punto2\\src\\MDSD-SentimentAnalisys.ipynb Cell 15\u001b[0m line \u001b[0;36m1\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/juanc/Documents/Repos/Uniandes/202320/NLP/NLP-Tarea3/Punto2/src/MDSD-SentimentAnalisys.ipynb#X24sZmlsZQ%3D%3D?line=127'>128</a>\u001b[0m doc_frec \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mcount_nonzero(tfs, axis\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/juanc/Documents/Repos/Uniandes/202320/NLP/NLP-Tarea3/Punto2/src/MDSD-SentimentAnalisys.ipynb#X24sZmlsZQ%3D%3D?line=128'>129</a>\u001b[0m total_docs \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(docs)\n\u001b[1;32m--> <a href='vscode-notebook-cell:/c%3A/Users/juanc/Documents/Repos/Uniandes/202320/NLP/NLP-Tarea3/Punto2/src/MDSD-SentimentAnalisys.ipynb#X24sZmlsZQ%3D%3D?line=129'>130</a>\u001b[0m tfidf \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mzeros((\u001b[39mlen\u001b[39m(docs), \u001b[39mlen\u001b[39m(voc)))\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/juanc/Documents/Repos/Uniandes/202320/NLP/NLP-Tarea3/Punto2/src/MDSD-SentimentAnalisys.ipynb#X24sZmlsZQ%3D%3D?line=130'>131</a>\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mlen\u001b[39m(docs)):\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/juanc/Documents/Repos/Uniandes/202320/NLP/NLP-Tarea3/Punto2/src/MDSD-SentimentAnalisys.ipynb#X24sZmlsZQ%3D%3D?line=131'>132</a>\u001b[0m     doc \u001b[39m=\u001b[39m docs\u001b[39m.\u001b[39miloc[i]\n",
      "\u001b[1;31mMemoryError\u001b[0m: Unable to allocate 7.39 GiB for an array with shape (8000, 123996) and data type float64"
     ]
    }
   ],
   "source": [
    "test_nv_classifier_complete()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlpEnv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
